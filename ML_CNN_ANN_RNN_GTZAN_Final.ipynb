{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWZMt7ZRFcB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import math\n",
        "import json\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "import keras\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN MODEL"
      ],
      "metadata": {
        "id": "LGCasCCznYnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = r\"/content/drive/MyDrive/DATASETS/GTZAN/Data/genres_original\"\n",
        "json_path = r\"data.json\"\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 30\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"
      ],
      "metadata": {
        "id": "tazVAglCFi5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_mfcc(dataset_path, json_path, n_mfcc=13, n_fft=2048,\n",
        "             hop_length=512, num_segments=5):\n",
        "    # Data storage dictionary\n",
        "    data = {\n",
        "        \"mapping\": [],\n",
        "        \"mfcc\": [],\n",
        "        \"labels\": [],\n",
        "    }\n",
        "    samples_ps = int(SAMPLES_PER_TRACK/num_segments) # ps = per segment\n",
        "    expected_vects_ps = math.ceil(samples_ps/hop_length)\n",
        "\n",
        "    # loop through all the genres\n",
        "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "        # ensuring not at root\n",
        "        if dirpath is not dataset_path:\n",
        "            # save the semantic label\n",
        "            dirpath_comp = dirpath.split(\"/\")\n",
        "            semantic_label = dirpath_comp[-1]\n",
        "            data[\"mapping\"].append(semantic_label)\n",
        "            print(f\"Processing: {semantic_label}\")\n",
        "\n",
        "            # process files for specific genre\n",
        "            for f in filenames:\n",
        "                if(f==str(\"jazz.00054.wav\")):\n",
        "                    # As librosa only read files <1Mb\n",
        "                    continue\n",
        "                else:\n",
        "                    # load audio file\n",
        "                    file_path = os.path.join(dirpath, f)\n",
        "                    signal,sr = librosa.load(file_path,sr=SAMPLE_RATE)\n",
        "                    for s in range(num_segments):\n",
        "                        start_sample = samples_ps * s\n",
        "                        finish_sample = start_sample + samples_ps\n",
        "\n",
        "                        mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],\n",
        "                                                    sr = sr,\n",
        "                                                    n_fft = n_fft,\n",
        "                                                    n_mfcc = n_mfcc,\n",
        "                                                    hop_length = hop_length)\n",
        "\n",
        "                        mfcc = mfcc.T\n",
        "\n",
        "                        # store mfcc if it has expected length\n",
        "                        if len(mfcc)==expected_vects_ps:\n",
        "                            data[\"mfcc\"].append(mfcc.tolist())\n",
        "                            data[\"labels\"].append(i-1)\n",
        "                            print(f\"{file_path}, segment: {s+1}\")\n",
        "\n",
        "    with open(json_path,\"w\") as f:\n",
        "        json.dump(data,f,indent=4)"
      ],
      "metadata": {
        "id": "i7KqDAwKGejy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from IPython.display import clear_output\n",
        "save_mfcc(dataset_path,json_path,num_segments=10)\n",
        "#clear_output()"
      ],
      "metadata": {
        "id": "WnqrplSNGg57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "def load_data(dataset_path):\n",
        "    with open(dataset_path,\"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert list to numpy arrays\n",
        "    inputs = np.array(data[\"mfcc\"])\n",
        "    targets = np.array(data[\"labels\"])\n",
        "\n",
        "    return inputs,targets"
      ],
      "metadata": {
        "id": "aWcC1EDjMH5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(test_size, validation_size):\n",
        "    X,y = load_data(r\"./data.json\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = validation_size)\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_val = X_val[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "metadata": {
        "id": "cPOB3ShDLvBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = prepare_dataset(0.25, 0.2)"
      ],
      "metadata": {
        "id": "fWxnLpeYLybw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (X_train.shape[1],X_train.shape[2],X_train.shape[3])\n",
        "print(input_shape)"
      ],
      "metadata": {
        "id": "V6PZ28v_MR-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), activation = \"relu\", input_shape = input_shape))\n",
        "model.add(MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
        "model.add(MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32, (2, 2), activation = \"relu\"))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(16, (1, 1), activation = \"relu\"))\n",
        "model.add(MaxPooling2D((1, 1), strides=(2, 2), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "#extra\n",
        "model.add(Conv2D(16, (1, 1), activation = \"relu\"))\n",
        "model.add(MaxPooling2D((1, 1), strides=(2, 2), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RLsFc_aiMXXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "hist = model.fit(X_train, y_train,\n",
        "                 validation_data = (X_val, y_val),\n",
        "                 epochs = 40,\n",
        "                 batch_size = 32)"
      ],
      "metadata": {
        "id": "9bTHKcLFRKST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(hist):\n",
        "    plt.figure(figsize=(20,15))\n",
        "    fig, axs = plt.subplots(2)\n",
        "    # accuracy subplot\n",
        "    axs[0].plot(hist.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(hist.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # Error subplot\n",
        "    axs[1].plot(hist.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(hist.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6I3sSpgSnQq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(hist)"
      ],
      "metadata": {
        "id": "CW4LZUMmThC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "uk8jud-ETh64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN MODEL:"
      ],
      "metadata": {
        "id": "v065zo-cKqb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "def load_data(dataset_path):\n",
        "    with open(dataset_path,\"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert list to numpy arrays\n",
        "    inputs = np.array(data[\"mfcc\"])\n",
        "    targets = np.array(data[\"labels\"])\n",
        "\n",
        "    return inputs,targets"
      ],
      "metadata": {
        "id": "lEKIYBjsKpWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs,targets = load_data(r\"./data.json\")"
      ],
      "metadata": {
        "id": "CxOOLcBxLEuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_train, input_test, target_train, target_test = train_test_split(inputs, targets, test_size=0.3)\n",
        "print(input_train.shape, target_train.shape)"
      ],
      "metadata": {
        "id": "iD0F4m_lLF5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape=(inputs.shape[1],inputs.shape[2])))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YOSQKuE-LI9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "             loss=\"sparse_categorical_crossentropy\",\n",
        "             metrics=[\"accuracy\"])\n",
        "\n",
        "hist = model.fit(input_train, target_train,\n",
        "                 validation_data = (input_test,target_test),\n",
        "                 epochs = 50,\n",
        "                 batch_size = 32)\n"
      ],
      "metadata": {
        "id": "GhOPnCktLWrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(hist):\n",
        "    plt.figure(figsize=(20,15))\n",
        "    fig, axs = plt.subplots(2)\n",
        "    # accuracy subplot\n",
        "    axs[0].plot(hist.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(hist.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # Error subplot\n",
        "    axs[1].plot(hist.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(hist.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lW4X0ZiYLb_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(hist)"
      ],
      "metadata": {
        "id": "PtFcNfUULfcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_error, test_accuracy = model.evaluate(input_test, target_test, verbose=1)\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "4xXjf6-hLk7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN MODEL:\n"
      ],
      "metadata": {
        "id": "2NE3FAbsdBak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "def load_data(dataset_path):\n",
        "    with open(dataset_path,\"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert list to numpy arrays\n",
        "    X = np.array(data[\"mfcc\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "\n",
        "    return X,y"
      ],
      "metadata": {
        "id": "bPsuJ6GOdWeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "AYf_Bx2pdocT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(test_size, validation_size):\n",
        "    X,y = load_data(r\"./data.json\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = validation_size)\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_val = X_val[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "metadata": {
        "id": "pNVBsVBldsE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape):\n",
        "    \"\"\"Generates RNN-LSTM model\n",
        "    :param input_shape (tuple): Shape of input set\n",
        "    :return model: RNN-LSTM model\n",
        "    \"\"\"\n",
        "\n",
        "    # build network topology\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 2 LSTM layers\n",
        "    model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(64))\n",
        "\n",
        "    # dense layer\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "\n",
        "    # dropout layer\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "gYkQnC2ad2xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # get train, validation, test splits\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_dataset(0.25, 0.2)\n",
        "\n",
        "    # create network\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2]) # 130, 13\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimiser = Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimiser,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "dK7vYZbkd-Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)"
      ],
      "metadata": {
        "id": "_zZIV1xor6Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # plot accuracy/error for training and validation\n",
        "    plot_history(history)\n"
      ],
      "metadata": {
        "id": "Njj3jms6sJW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # evaluate model on test set\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print('\\nTest accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "sSewu4gEsNEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}